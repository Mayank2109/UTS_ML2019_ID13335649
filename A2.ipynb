{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayank2109/UTS_ML2019_ID13335649/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t05tQuKLnmKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUyPl8XlovRe"
      },
      "source": [
        "# Review Report on Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection\n",
        "\n",
        "#Introduction\n",
        "\n",
        "This report reviews and critiques the paper “Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection” written by Peter N. Belhumeur, Joao P. Hespanha, and David J. Kriegman.\n",
        "\n",
        "#Content\n",
        "\n",
        "The research proposes a Fisherfaces approach that is quite efficient in face recognition while in various conditions like large variations in facial expression and lighting directions. This paper addresses the complications of face recognition under large variations in lighting, facial expressions.  Usually, the image of size x$*$y pixels are represented by vector in x$*$y dimensional space.  But this high dimensional image space is reduced to a much smaller dimension space taking into account that different human faces have similarities and thus can be settled to smaller dimension space. Particularly, the Fisherfaces approach uses the fisher's linear discriminant   (FLD) where the learning dataset is labelled and hence uses the class information (supervised learning) which further adds into the discriminant power. Liu, H., (2014) mentions that class specific linear method projects the vectors in a space such that feature vectors within the same class of images move nearer and those from different classes are spread apart. Hence this approach produces high discriminability in large variations and therefore have lower error rates during face recognition as compared to other methods discussed. \n",
        "\n",
        "Furthermore, methods like correlation, linear subspaces and Eigenfaces are also compared and explained. Specifically, the Eigenfaces and Fisherfaces methods are explained at greater depth with the support of mathematical equations. The Eigenfaces approach utilizes principal component analysis (PCA) on the image dataset. This technique also aims for dimensionality reduction by finding principal components or eigenvectors of the dataset. But this method maximizes the scatter among all the classes (along with within-class scatter) which remains susceptible to variations like lightning and facial expressions. This research describes that this drawback is overcome by the fisherface approach by maximizing the between-class scatter but minimizing the within-class scatter that demonstrates better discrimination power of FLD over PCA. The experiments are also discussed that tested the performance of all the algorithms by using image database from the Harvard Robotics Laboratory. The results shows that among all the algorithms, fisherfaces performs the best under variations in lighting and face expressions. \n",
        "\n",
        "#Innovation\n",
        "\n",
        "The representation of the image training dataset is significant for classifying the test image. As such, this research addresses how the image dataset should be reconstructed in order to achieve an optimal projection matrix which is further utilized for classification. Fisherface uses FLD technique to retain class discriminatory data as much as possible. As such, it generates a linear combination of its vectors by maximizing the differences between the means of the classes and thereby normalizing the within-class scatter. It defines two measures i.e between-class scatter matrix as\n",
        "\n",
        "$$ S_B =   \\sum_{i=1}^c N_i (\\mu_i- \\mu)(\\mu_i- \\mu)^T $$\n",
        "\n",
        "and within-class scatter matrix as \n",
        "\n",
        "$$ S_W =   \\sum_{i=1}^c \\sum_{x_k \\epsilon X_i }  (x_k-\\mu_i )(x_k-\\mu_i)^T $$\n",
        "\n",
        "where N is the total number of sample images, c is the total classes, $\\mu$ is the the mean image of the all samples and $\\mu_i$ is the mean image of class $x_i$.\n",
        "This approach seeks to maximize the between-class measure and simultaneously minimize the within class measure. Therefore, the optimal projection matrix $W_{opt}$ is generated with orthogonal columns by maximizing the ratio of between-class scatter and within-class scatter of dataset. \n",
        "\n",
        "$$W_{opt}= \\arg \\max \\frac{W^T S_B W}{W^TS_W W}$$\n",
        "\n",
        "That is used to produce the eigenvectors and their respective eigen values. The further research clearly demonstrates the efficiency of LDA technique over PCA technique. \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/Mayank2109/UTS_ML2019_ID13335649/blob/master/PCA%26LA.png?raw=true\"   width=\"250\"/> \n",
        "\n",
        "Fig 1: Visualization of FLD and PCA projections for sample vectors of two classes of dataset.\n",
        "\n",
        "The figure above shows the dataset samples from two classes that is situated near the linear subspace. Here it compares the projections constructed by two techniques i.e FLD and PCA. The PCA actually projects the locations of the classes together where it makes it harder to differentiate and therefore appears inseparable. Whereas, FLD projects these classes in a way that is easier to differentiate and hence provides effective discrimination power and high recognition rates.\n",
        "\n",
        "Moreover, one of the challenges is to overcome the situation where the within-class scatter is singular. This research addresses this situation by projecting the learning dataset to a smaller dimension space where within-class scatter is non-singular.\n",
        "\n",
        "It is attained by using PCA technique to shrink the dimension of the feature space to N - c, and thereby utilizing the FLD technique to shrink the dimension to c-1 so that at most c-1 nonzero eigenvectors exists.\n",
        "Therefore, $W_{opt}$ is defined by\n",
        "\n",
        "$$W_{opt}^T=W_{fld}^T W_{pca}^T$$\n",
        "\n",
        "where \n",
        "\n",
        "$$W_{pca}= \\arg \\max | W^T S_T W|$$\n",
        "\n",
        "\n",
        "$$W_{fld}= \\arg \\max \\frac{|W^T W_{pca}^T S_B W_{pca} W|}{|W^T W_{pca}^T S_W W_{pca} W|}$$\n",
        "\n",
        "\n",
        "#Technical quality\n",
        "\n",
        "The research paper at that time provided with rich insights and in-depth knowledge about the various methods (specially Fisherfaces and Eigenfaces approach) that were applied for face recognition. Description of respective methods were mentioned along with their intrinsic properties (their strong and weak components) that affects the recognition rates and moreover it also states the limitation of such face recognition methods that is affected by variations like lightning and face expressions. The proposed approach and Eigenfaces method were explained with support of statistical equations.\n",
        "\n",
        "Furthermore, the classification of such methods were compared and measured based on the recognition error rates percentages. This comparison facilitates the understanding of the effectiveness of training datasets generated by respective methods.  The experiments were also carried out separately for light variations and variations in face expression and eyewear. This ensured that the performance of algorithms based on one variation should not be affected by the other variation. The visual representation (graph and tables) of the recognition error rates for each methods were presented well when trained for different data subsets.  However, this research was not able to report about a situation where the learning dataset is considerably small such that it leads to better recognition rates for PLA technique rather than FLD technique. This condition is examined by Martinez et al. (2000).\n",
        "\n",
        "#Application and X-factor\n",
        "\n",
        "The proposed algorithm can significantly be used for pattern recognition and machine learning domain. Here the underlying base is the representation of data for the classification purpose. As such, the Fisherface approach utilizes linear discriminant analysis (LDA) technique that seeks a subspace which map the sample vectors from different classes further way and sample vectors within the class closer to strengthen the feature representation (Martinez, A., 2011). Thus it simplifies the classification process than any other methods discussed.\n",
        "\n",
        "LDA technique is also applied in bankruptcy prediction (Linear discriminant analysis Wiki) where this statistical method is utilized to anticipate whether any firm has tendency to get bankrupt or is capable to survive. The model is also utilized in biomedicine domain to evaluate the progression of disease in a patient. It is also used to classify different biological organisms.\n",
        "\n",
        "The fisherfaces extensions exist in other research works as well. Yang et al. (2005) proposes Two dimensional LDA (2LDA), which is the extension of LDA. Where the LDA reduces the input dataset to low dimension vector, the 2LDA seeks for the specific features from the input dataset based on LDA technique. As such, this approach focusses on discriminant features from the corners of the image. The findings from such research proves that 2LDA performs better than LDA for recognition and feature extraction.\n",
        "\n",
        "I found that this research work by Belhumeur et al. (1997) emphasizes on the representation of the dataset and how effective it can be for better classification results. Therefore, I believe this research is quite interesting to discuss with peers to broaden our knowledge in machine learning as well as deep learning.\n",
        "\n",
        "#Presentation\n",
        "\n",
        "The research is organized thoroughly and presented quite clearly. The content of the research and the discussions were backed and supported with other referred research work and also with mathematical equations. So, I found less difficulties in following up with the arguments as justifications were provided adequately. In some cases, where the meaning of some terminologies were unknown, so further research was required.  The research work is presented systematically with the description of methods along with explanation of the formation of datasets, and eventually the experiments results were mentioned with appropriate tables and graphs. Therefore, the quality of the research work is quite in-depth which reflects various significant insights in machine learning and face recognition domain.\n",
        "\n",
        "#References\n",
        "\n",
        "Martínez, A.M. and Kak, A.C., 2001. Pca versus lda. IEEE transactions on pattern analysis and machine intelligence, 23(2), pp.228-233.\n",
        "\n",
        "Yang, J., Zhang, D., Yong, X. and Yang, J.Y., 2005. Two-dimensional discriminant transform for face recognition. Pattern recognition, 38(7), pp.1125-1129.\n",
        "\n",
        "Liu, H., 2014. Face Detection and Recognition on Mobile Devices. Elsevier.\n",
        "\n",
        "Martinez, A., 2011. Fisherfaces. Scholarpedia, viewed 26 August 2019, < https://www.scholarpedia.org/article/Fisherfaces>\n",
        "\n",
        "Linear discriminant analysis. Wikipedia, The Free Encyclopedia, viewed 26 August 2019, < https://en.wikipedia.org/w/index.php?title=Linear_discriminant_analysis&oldid=911812623>\n",
        "\n",
        "Github link of review report- https://github.com/Mayank2109/UTS_ML2019_ID13335649/blob/master/A1.ipynb\n",
        "\n"
      ]
    }
  ]
}